{"cells":[{"cell_type":"markdown","metadata":{"id":"6YRm2rhBlm9J"},"source":["# Next Word Prediction:"]},{"cell_type":"markdown","metadata":{"id":"9Rxw4F3Qlm9Q"},"source":["### Importing The Required Libraries:"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4193,"status":"ok","timestamp":1663996123582,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"Ex25PMQtlm9S"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","import pickle\n","import numpy as np\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1663996123584,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"CiyNOXJIlm9V","outputId":"983994b8-4c27-456b-ceee-eb22824a5aad"},"outputs":[{"name":"stdout","output_type":"stream","text":["The First Line:  ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n","\n","The Last Line:  first to get up and stretch out her young body.\n"]}],"source":["\"\"\"\n","    Dataset: http://www.gutenberg.org/cache/epub/5200/pg5200.txt\n","    Remove all the unnecessary data and label it as Metamorphosis-clean.\n","    The starting and ending lines should be as follows.\n","\n","\"\"\"\n","\n","\n","file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n","lines = []\n","\n","for i in file:\n","    lines.append(i)\n","    \n","print(\"The First Line: \", lines[0])\n","print(\"The Last Line: \", lines[-1])"]},{"cell_type":"markdown","metadata":{"id":"i9i_vCUklm9Y"},"source":["### Cleaning the data:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"elapsed":365,"status":"ok","timestamp":1663996123939,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"dSbAddHQlm9Y","outputId":"d82aaf66-921d-4e35-85fb-0ecf70abf2d1"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data = \"\"\n","\n","for i in lines:\n","    data = ' '. join(lines)\n","    \n","data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n","data[:360]"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1663996123941,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"acBk19d_lm9a","outputId":"a1c4f9cf-0f35-4b14-b01a-c5b75121eeb6"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import string\n","\n","translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n","new_data = data.translate(translator)\n","\n","new_data[:500]"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":327,"status":"ok","timestamp":1663996124255,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"5NTYzdUGlm9b","outputId":"9194a031-4a75-44f0-abe3-21e904fd54ac"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["z = []\n","\n","for i in data.split():\n","    if i not in z:\n","        z.append(i)\n","        \n","data = ' '.join(z)\n","data[:500]"]},{"cell_type":"markdown","metadata":{"id":"6PfEptu9lm9d"},"source":["### Tokenization:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1663996124256,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"0fMIx91Rlm9e","outputId":"cc234282-dfe4-4922-bad0-d3c030456e9e"},"outputs":[{"data":{"text/plain":["[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([data])\n","\n","# saving the tokenizer for predict function.\n","pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n","\n","sequence_data = tokenizer.texts_to_sequences([data])[0]\n","sequence_data[:10]"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1663996124258,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"GjFtkFv6lm9f","outputId":"75376c08-8e1d-4be7-d247-dcc41caa6b69"},"outputs":[{"name":"stdout","output_type":"stream","text":["2617\n"]}],"source":["vocab_size = len(tokenizer.word_index) + 1\n","print(vocab_size)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1663996124261,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"uXDiOa6flm9f","outputId":"2e105c2b-42b0-4dd8-f6b5-840f14acd7ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Length of sequences are:  3889\n"]},{"data":{"text/plain":["array([[ 17,  53],\n","       [ 53, 293],\n","       [293,   2],\n","       [  2,  18],\n","       [ 18, 729],\n","       [729, 135],\n","       [135, 730],\n","       [730, 294],\n","       [294,   8],\n","       [  8, 731]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["sequences = []\n","\n","for i in range(1, len(sequence_data)):\n","    words = sequence_data[i-1:i+1]\n","    sequences.append(words)\n","    \n","print(\"The Length of sequences are: \", len(sequences))\n","sequences = np.array(sequences)\n","sequences[:10]"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1663996124262,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"oaSdvy1_lm9g"},"outputs":[],"source":["X = []\n","y = []\n","\n","for i in sequences:\n","    X.append(i[0])\n","    y.append(i[1])\n","    \n","X = np.array(X)\n","y = np.array(y)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1663996124501,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"VcR_Vb_blm9h","outputId":"050c8af8-0246-4622-8656-0f6ef70641f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Data is:  [ 17  53 293   2  18]\n","The responses are:  [ 53 293   2  18 729]\n"]}],"source":["print(\"The Data is: \", X[:5])\n","print(\"The responses are: \", y[:5])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1663996124502,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"UjQN2Z7Vlm9i","outputId":"ce68a41d-2c5a-4d5f-8906-6b47813ca408"},"outputs":[{"data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 1., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["y = to_categorical(y, num_classes=vocab_size)\n","y[:5]"]},{"cell_type":"markdown","metadata":{"id":"Q8bLXh94lm9k"},"source":["### Creating the Model:"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3078,"status":"ok","timestamp":1663996127573,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"VT0dcavAlm9l"},"outputs":[],"source":["model = Sequential()\n","model.add(Embedding(vocab_size, 10, input_length=1))\n","model.add(LSTM(1000, return_sequences=True))\n","model.add(LSTM(1000))\n","model.add(Dense(1000, activation=\"relu\"))\n","model.add(Dense(vocab_size, activation=\"softmax\"))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1663996127574,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"ruh78srKlm9m","outputId":"5b402de0-7391-4e70-d7e0-29b7311061ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 1, 10)             26170     \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 1000)           4044000   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1000)              8004000   \n","                                                                 \n"," dense (Dense)               (None, 1000)              1001000   \n","                                                                 \n"," dense_1 (Dense)             (None, 2617)              2619617   \n","                                                                 \n","=================================================================\n","Total params: 15,694,787\n","Trainable params: 15,694,787\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"kEgmXdpOlm9n"},"source":["### Plot The Model:"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":302,"status":"ok","timestamp":1663996127868,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"r-eIg-6Ilm9n","outputId":"b6c4027a-faae-4227-d5bd-01f75e0677eb"},"outputs":[{"data":{"image/png":"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\n","text/plain":["\u003cIPython.core.display.Image object\u003e"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow import keras\n","from keras.utils.vis_utils import plot_model\n","\n","keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"]},{"cell_type":"markdown","metadata":{"id":"vVEU2GJ5lm9o"},"source":["### Callbacks:"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1663996127870,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"9XXd-68xlm9p"},"outputs":[],"source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n","    save_best_only=True, mode='auto')\n","\n","reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n","\n","logdir='logsnextword1'\n","tensorboard_Visualization = TensorBoard(log_dir=logdir)"]},{"cell_type":"markdown","metadata":{"id":"L08S7I8slm9p"},"source":["### Compile The Model:"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1663996127871,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"r13jxWt_lm9q","outputId":"7757d167-c487-4373-ffc9-5f0a0136a3dc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]}],"source":["model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"]},{"cell_type":"markdown","metadata":{"id":"ht7zQW3Slm9q"},"source":["### Fit The Model:"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2305794,"status":"ok","timestamp":1663998433653,"user":{"displayName":"sophie white","userId":"12191687335629451267"},"user_tz":240},"id":"UTo-mxuilm9q","outputId":"3666241b-bb06-4a35-e342-1cfcd9c6fbe8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/150\n","61/61 [==============================] - ETA: 0s - loss: 7.8752\n","Epoch 1: loss improved from inf to 7.87519, saving model to nextword1.h5\n","61/61 [==============================] - 19s 242ms/step - loss: 7.8752 - lr: 0.0010\n","Epoch 2/150\n","61/61 [==============================] - ETA: 0s - loss: 7.8592\n","Epoch 2: loss improved from 7.87519 to 7.85917, saving model to nextword1.h5\n","61/61 [==============================] - 15s 250ms/step - loss: 7.8592 - lr: 0.0010\n","Epoch 3/150\n","61/61 [==============================] - ETA: 0s - loss: 7.8257\n","Epoch 3: loss improved from 7.85917 to 7.82572, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 7.8257 - lr: 0.0010\n","Epoch 4/150\n","61/61 [==============================] - ETA: 0s - loss: 7.6626\n","Epoch 4: loss improved from 7.82572 to 7.66263, saving model to nextword1.h5\n","61/61 [==============================] - 14s 236ms/step - loss: 7.6626 - lr: 0.0010\n","Epoch 5/150\n","61/61 [==============================] - ETA: 0s - loss: 7.4659\n","Epoch 5: loss improved from 7.66263 to 7.46586, saving model to nextword1.h5\n","61/61 [==============================] - 14s 237ms/step - loss: 7.4659 - lr: 0.0010\n","Epoch 6/150\n","61/61 [==============================] - ETA: 0s - loss: 7.2927\n","Epoch 6: loss improved from 7.46586 to 7.29269, saving model to nextword1.h5\n","61/61 [==============================] - 15s 241ms/step - loss: 7.2927 - lr: 0.0010\n","Epoch 7/150\n","61/61 [==============================] - ETA: 0s - loss: 7.1674\n","Epoch 7: loss improved from 7.29269 to 7.16739, saving model to nextword1.h5\n","61/61 [==============================] - 16s 260ms/step - loss: 7.1674 - lr: 0.0010\n","Epoch 8/150\n","61/61 [==============================] - ETA: 0s - loss: 7.0313\n","Epoch 8: loss improved from 7.16739 to 7.03128, saving model to nextword1.h5\n","61/61 [==============================] - 15s 239ms/step - loss: 7.0313 - lr: 0.0010\n","Epoch 9/150\n","61/61 [==============================] - ETA: 0s - loss: 6.8307\n","Epoch 9: loss improved from 7.03128 to 6.83071, saving model to nextword1.h5\n","61/61 [==============================] - 15s 243ms/step - loss: 6.8307 - lr: 0.0010\n","Epoch 10/150\n","61/61 [==============================] - ETA: 0s - loss: 6.5638\n","Epoch 10: loss improved from 6.83071 to 6.56377, saving model to nextword1.h5\n","61/61 [==============================] - 15s 250ms/step - loss: 6.5638 - lr: 0.0010\n","Epoch 11/150\n","61/61 [==============================] - ETA: 0s - loss: 6.3035\n","Epoch 11: loss improved from 6.56377 to 6.30355, saving model to nextword1.h5\n","61/61 [==============================] - 15s 238ms/step - loss: 6.3035 - lr: 0.0010\n","Epoch 12/150\n","61/61 [==============================] - ETA: 0s - loss: 6.0921\n","Epoch 12: loss improved from 6.30355 to 6.09205, saving model to nextword1.h5\n","61/61 [==============================] - 15s 239ms/step - loss: 6.0921 - lr: 0.0010\n","Epoch 13/150\n","61/61 [==============================] - ETA: 0s - loss: 5.8848\n","Epoch 13: loss improved from 6.09205 to 5.88484, saving model to nextword1.h5\n","61/61 [==============================] - 14s 236ms/step - loss: 5.8848 - lr: 0.0010\n","Epoch 14/150\n","61/61 [==============================] - ETA: 0s - loss: 5.6745\n","Epoch 14: loss improved from 5.88484 to 5.67449, saving model to nextword1.h5\n","61/61 [==============================] - 16s 261ms/step - loss: 5.6745 - lr: 0.0010\n","Epoch 15/150\n","61/61 [==============================] - ETA: 0s - loss: 5.4512\n","Epoch 15: loss improved from 5.67449 to 5.45122, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 5.4512 - lr: 0.0010\n","Epoch 16/150\n","61/61 [==============================] - ETA: 0s - loss: 5.2398\n","Epoch 16: loss improved from 5.45122 to 5.23979, saving model to nextword1.h5\n","61/61 [==============================] - 15s 238ms/step - loss: 5.2398 - lr: 0.0010\n","Epoch 17/150\n","61/61 [==============================] - ETA: 0s - loss: 5.0079\n","Epoch 17: loss improved from 5.23979 to 5.00789, saving model to nextword1.h5\n","61/61 [==============================] - 15s 240ms/step - loss: 5.0079 - lr: 0.0010\n","Epoch 18/150\n","61/61 [==============================] - ETA: 0s - loss: 4.7515\n","Epoch 18: loss improved from 5.00789 to 4.75145, saving model to nextword1.h5\n","61/61 [==============================] - 15s 238ms/step - loss: 4.7515 - lr: 0.0010\n","Epoch 19/150\n","61/61 [==============================] - ETA: 0s - loss: 4.5053\n","Epoch 19: loss improved from 4.75145 to 4.50530, saving model to nextword1.h5\n","61/61 [==============================] - 15s 241ms/step - loss: 4.5053 - lr: 0.0010\n","Epoch 20/150\n","61/61 [==============================] - ETA: 0s - loss: 4.3226\n","Epoch 20: loss improved from 4.50530 to 4.32263, saving model to nextword1.h5\n","61/61 [==============================] - 15s 238ms/step - loss: 4.3226 - lr: 0.0010\n","Epoch 21/150\n","61/61 [==============================] - ETA: 0s - loss: 4.1097\n","Epoch 21: loss improved from 4.32263 to 4.10975, saving model to nextword1.h5\n","61/61 [==============================] - 15s 238ms/step - loss: 4.1097 - lr: 0.0010\n","Epoch 22/150\n","61/61 [==============================] - ETA: 0s - loss: 3.9410\n","Epoch 22: loss improved from 4.10975 to 3.94098, saving model to nextword1.h5\n","61/61 [==============================] - 16s 261ms/step - loss: 3.9410 - lr: 0.0010\n","Epoch 23/150\n","61/61 [==============================] - ETA: 0s - loss: 3.7886\n","Epoch 23: loss improved from 3.94098 to 3.78860, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 3.7886 - lr: 0.0010\n","Epoch 24/150\n","61/61 [==============================] - ETA: 0s - loss: 3.6574\n","Epoch 24: loss improved from 3.78860 to 3.65740, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 3.6574 - lr: 0.0010\n","Epoch 25/150\n","61/61 [==============================] - ETA: 0s - loss: 3.4943\n","Epoch 25: loss improved from 3.65740 to 3.49430, saving model to nextword1.h5\n","61/61 [==============================] - 15s 240ms/step - loss: 3.4943 - lr: 0.0010\n","Epoch 26/150\n","61/61 [==============================] - ETA: 0s - loss: 3.3864\n","Epoch 26: loss improved from 3.49430 to 3.38641, saving model to nextword1.h5\n","61/61 [==============================] - 15s 241ms/step - loss: 3.3864 - lr: 0.0010\n","Epoch 27/150\n","61/61 [==============================] - ETA: 0s - loss: 3.2381\n","Epoch 27: loss improved from 3.38641 to 3.23807, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 3.2381 - lr: 0.0010\n","Epoch 28/150\n","61/61 [==============================] - ETA: 0s - loss: 3.1296\n","Epoch 28: loss improved from 3.23807 to 3.12961, saving model to nextword1.h5\n","61/61 [==============================] - 14s 236ms/step - loss: 3.1296 - lr: 0.0010\n","Epoch 29/150\n","61/61 [==============================] - ETA: 0s - loss: 3.0520\n","Epoch 29: loss improved from 3.12961 to 3.05202, saving model to nextword1.h5\n","61/61 [==============================] - 15s 249ms/step - loss: 3.0520 - lr: 0.0010\n","Epoch 30/150\n","61/61 [==============================] - ETA: 0s - loss: 2.9663\n","Epoch 30: loss improved from 3.05202 to 2.96628, saving model to nextword1.h5\n","61/61 [==============================] - 16s 262ms/step - loss: 2.9663 - lr: 0.0010\n","Epoch 31/150\n","61/61 [==============================] - ETA: 0s - loss: 2.8650\n","Epoch 31: loss improved from 2.96628 to 2.86500, saving model to nextword1.h5\n","61/61 [==============================] - 15s 250ms/step - loss: 2.8650 - lr: 0.0010\n","Epoch 32/150\n","61/61 [==============================] - ETA: 0s - loss: 2.8029\n","Epoch 32: loss improved from 2.86500 to 2.80286, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 2.8029 - lr: 0.0010\n","Epoch 33/150\n","61/61 [==============================] - ETA: 0s - loss: 2.7564\n","Epoch 33: loss improved from 2.80286 to 2.75638, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 2.7564 - lr: 0.0010\n","Epoch 34/150\n","61/61 [==============================] - ETA: 0s - loss: 2.6836\n","Epoch 34: loss improved from 2.75638 to 2.68358, saving model to nextword1.h5\n","61/61 [==============================] - 15s 239ms/step - loss: 2.6836 - lr: 0.0010\n","Epoch 35/150\n","61/61 [==============================] - ETA: 0s - loss: 2.6053\n","Epoch 35: loss improved from 2.68358 to 2.60528, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 2.6053 - lr: 0.0010\n","Epoch 36/150\n","61/61 [==============================] - ETA: 0s - loss: 2.5369\n","Epoch 36: loss improved from 2.60528 to 2.53689, saving model to nextword1.h5\n","61/61 [==============================] - 15s 253ms/step - loss: 2.5369 - lr: 0.0010\n","Epoch 37/150\n","61/61 [==============================] - ETA: 0s - loss: 2.4668\n","Epoch 37: loss improved from 2.53689 to 2.46681, saving model to nextword1.h5\n","61/61 [==============================] - 15s 239ms/step - loss: 2.4668 - lr: 0.0010\n","Epoch 38/150\n","61/61 [==============================] - ETA: 0s - loss: 2.4322\n","Epoch 38: loss improved from 2.46681 to 2.43220, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 2.4322 - lr: 0.0010\n","Epoch 39/150\n","61/61 [==============================] - ETA: 0s - loss: 2.4021\n","Epoch 39: loss improved from 2.43220 to 2.40206, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 2.4021 - lr: 0.0010\n","Epoch 40/150\n","61/61 [==============================] - ETA: 0s - loss: 2.3191\n","Epoch 40: loss improved from 2.40206 to 2.31912, saving model to nextword1.h5\n","61/61 [==============================] - 15s 250ms/step - loss: 2.3191 - lr: 0.0010\n","Epoch 41/150\n","61/61 [==============================] - ETA: 0s - loss: 2.2595\n","Epoch 41: loss improved from 2.31912 to 2.25950, saving model to nextword1.h5\n","61/61 [==============================] - 16s 263ms/step - loss: 2.2595 - lr: 0.0010\n","Epoch 42/150\n","61/61 [==============================] - ETA: 0s - loss: 2.2260\n","Epoch 42: loss improved from 2.25950 to 2.22601, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 2.2260 - lr: 0.0010\n","Epoch 43/150\n","61/61 [==============================] - ETA: 0s - loss: 2.1813\n","Epoch 43: loss improved from 2.22601 to 2.18133, saving model to nextword1.h5\n","61/61 [==============================] - 15s 245ms/step - loss: 2.1813 - lr: 0.0010\n","Epoch 44/150\n","61/61 [==============================] - ETA: 0s - loss: 2.1444\n","Epoch 44: loss improved from 2.18133 to 2.14442, saving model to nextword1.h5\n","61/61 [==============================] - 15s 250ms/step - loss: 2.1444 - lr: 0.0010\n","Epoch 45/150\n","61/61 [==============================] - ETA: 0s - loss: 2.0911\n","Epoch 45: loss improved from 2.14442 to 2.09105, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 2.0911 - lr: 0.0010\n","Epoch 46/150\n","61/61 [==============================] - ETA: 0s - loss: 2.0664\n","Epoch 46: loss improved from 2.09105 to 2.06642, saving model to nextword1.h5\n","61/61 [==============================] - 15s 241ms/step - loss: 2.0664 - lr: 0.0010\n","Epoch 47/150\n","61/61 [==============================] - ETA: 0s - loss: 2.0401\n","Epoch 47: loss improved from 2.06642 to 2.04014, saving model to nextword1.h5\n","61/61 [==============================] - 15s 251ms/step - loss: 2.0401 - lr: 0.0010\n","Epoch 48/150\n","61/61 [==============================] - ETA: 0s - loss: 2.0091\n","Epoch 48: loss improved from 2.04014 to 2.00907, saving model to nextword1.h5\n","61/61 [==============================] - 14s 237ms/step - loss: 2.0091 - lr: 0.0010\n","Epoch 49/150\n","61/61 [==============================] - ETA: 0s - loss: 1.9422\n","Epoch 49: loss improved from 2.00907 to 1.94225, saving model to nextword1.h5\n","61/61 [==============================] - 14s 237ms/step - loss: 1.9422 - lr: 0.0010\n","Epoch 50/150\n","61/61 [==============================] - ETA: 0s - loss: 1.9060\n","Epoch 50: loss improved from 1.94225 to 1.90599, saving model to nextword1.h5\n","61/61 [==============================] - 15s 243ms/step - loss: 1.9060 - lr: 0.0010\n","Epoch 51/150\n","61/61 [==============================] - ETA: 0s - loss: 1.8690\n","Epoch 51: loss improved from 1.90599 to 1.86899, saving model to nextword1.h5\n","61/61 [==============================] - 16s 257ms/step - loss: 1.8690 - lr: 0.0010\n","Epoch 52/150\n","61/61 [==============================] - ETA: 0s - loss: 1.8356\n","Epoch 52: loss improved from 1.86899 to 1.83562, saving model to nextword1.h5\n","61/61 [==============================] - 15s 247ms/step - loss: 1.8356 - lr: 0.0010\n","Epoch 53/150\n","61/61 [==============================] - ETA: 0s - loss: 1.8202\n","Epoch 53: loss improved from 1.83562 to 1.82020, saving model to nextword1.h5\n","61/61 [==============================] - 15s 243ms/step - loss: 1.8202 - lr: 0.0010\n","Epoch 54/150\n","61/61 [==============================] - ETA: 0s - loss: 1.8076\n","Epoch 54: loss improved from 1.82020 to 1.80756, saving model to nextword1.h5\n","61/61 [==============================] - 15s 245ms/step - loss: 1.8076 - lr: 0.0010\n","Epoch 55/150\n","61/61 [==============================] - ETA: 0s - loss: 1.7759\n","Epoch 55: loss improved from 1.80756 to 1.77590, saving model to nextword1.h5\n","61/61 [==============================] - 15s 240ms/step - loss: 1.7759 - lr: 0.0010\n","Epoch 56/150\n","61/61 [==============================] - ETA: 0s - loss: 1.7508\n","Epoch 56: loss improved from 1.77590 to 1.75075, saving model to nextword1.h5\n","61/61 [==============================] - 15s 238ms/step - loss: 1.7508 - lr: 0.0010\n","Epoch 57/150\n","61/61 [==============================] - ETA: 0s - loss: 1.7235\n","Epoch 57: loss improved from 1.75075 to 1.72351, saving model to nextword1.h5\n","61/61 [==============================] - 15s 238ms/step - loss: 1.7235 - lr: 0.0010\n","Epoch 58/150\n","61/61 [==============================] - ETA: 0s - loss: 1.7077\n","Epoch 58: loss improved from 1.72351 to 1.70765, saving model to nextword1.h5\n","61/61 [==============================] - 16s 263ms/step - loss: 1.7077 - lr: 0.0010\n","Epoch 59/150\n","61/61 [==============================] - ETA: 0s - loss: 1.6639\n","Epoch 59: loss improved from 1.70765 to 1.66394, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 1.6639 - lr: 0.0010\n","Epoch 60/150\n","61/61 [==============================] - ETA: 0s - loss: 1.6446\n","Epoch 60: loss improved from 1.66394 to 1.64459, saving model to nextword1.h5\n","61/61 [==============================] - 15s 239ms/step - loss: 1.6446 - lr: 0.0010\n","Epoch 61/150\n","61/61 [==============================] - ETA: 0s - loss: 1.6304\n","Epoch 61: loss improved from 1.64459 to 1.63041, saving model to nextword1.h5\n","61/61 [==============================] - 15s 238ms/step - loss: 1.6304 - lr: 0.0010\n","Epoch 62/150\n","61/61 [==============================] - ETA: 0s - loss: 1.6200\n","Epoch 62: loss improved from 1.63041 to 1.62002, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 1.6200 - lr: 0.0010\n","Epoch 63/150\n","61/61 [==============================] - ETA: 0s - loss: 1.5728\n","Epoch 63: loss improved from 1.62002 to 1.57275, saving model to nextword1.h5\n","61/61 [==============================] - 15s 246ms/step - loss: 1.5728 - lr: 0.0010\n","Epoch 64/150\n","61/61 [==============================] - ETA: 0s - loss: 1.5360\n","Epoch 64: loss improved from 1.57275 to 1.53604, saving model to nextword1.h5\n","61/61 [==============================] - 15s 239ms/step - loss: 1.5360 - lr: 0.0010\n","Epoch 65/150\n","61/61 [==============================] - ETA: 0s - loss: 1.5252\n","Epoch 65: loss improved from 1.53604 to 1.52520, saving model to nextword1.h5\n","61/61 [==============================] - 15s 253ms/step - loss: 1.5252 - lr: 0.0010\n","Epoch 66/150\n","61/61 [==============================] - ETA: 0s - loss: 1.5010\n","Epoch 66: loss improved from 1.52520 to 1.50098, saving model to nextword1.h5\n","61/61 [==============================] - 16s 255ms/step - loss: 1.5010 - lr: 0.0010\n","Epoch 67/150\n","61/61 [==============================] - ETA: 0s - loss: 1.4982\n","Epoch 67: loss improved from 1.50098 to 1.49816, saving model to nextword1.h5\n","61/61 [==============================] - 15s 246ms/step - loss: 1.4982 - lr: 0.0010\n","Epoch 68/150\n","61/61 [==============================] - ETA: 0s - loss: 1.4777\n","Epoch 68: loss improved from 1.49816 to 1.47767, saving model to nextword1.h5\n","61/61 [==============================] - 15s 243ms/step - loss: 1.4777 - lr: 0.0010\n","Epoch 69/150\n","61/61 [==============================] - ETA: 0s - loss: 1.4289\n","Epoch 69: loss improved from 1.47767 to 1.42895, saving model to nextword1.h5\n","61/61 [==============================] - 14s 236ms/step - loss: 1.4289 - lr: 0.0010\n","Epoch 70/150\n","61/61 [==============================] - ETA: 0s - loss: 1.4334\n","Epoch 70: loss did not improve from 1.42895\n","61/61 [==============================] - 14s 236ms/step - loss: 1.4334 - lr: 0.0010\n","Epoch 71/150\n","61/61 [==============================] - ETA: 0s - loss: 1.4392\n","Epoch 71: loss did not improve from 1.42895\n","61/61 [==============================] - 15s 252ms/step - loss: 1.4392 - lr: 0.0010\n","Epoch 72/150\n","61/61 [==============================] - ETA: 0s - loss: 1.4274\n","Epoch 72: loss improved from 1.42895 to 1.42741, saving model to nextword1.h5\n","61/61 [==============================] - 15s 246ms/step - loss: 1.4274 - lr: 0.0010\n","Epoch 73/150\n","61/61 [==============================] - ETA: 0s - loss: 1.3778\n","Epoch 73: loss improved from 1.42741 to 1.37784, saving model to nextword1.h5\n","61/61 [==============================] - 16s 264ms/step - loss: 1.3778 - lr: 0.0010\n","Epoch 74/150\n","61/61 [==============================] - ETA: 0s - loss: 1.3356\n","Epoch 74: loss improved from 1.37784 to 1.33556, saving model to nextword1.h5\n","61/61 [==============================] - 15s 253ms/step - loss: 1.3356 - lr: 0.0010\n","Epoch 75/150\n","61/61 [==============================] - ETA: 0s - loss: 1.3005\n","Epoch 75: loss improved from 1.33556 to 1.30053, saving model to nextword1.h5\n","61/61 [==============================] - 15s 246ms/step - loss: 1.3005 - lr: 0.0010\n","Epoch 76/150\n","61/61 [==============================] - ETA: 0s - loss: 1.2690\n","Epoch 76: loss improved from 1.30053 to 1.26900, saving model to nextword1.h5\n","61/61 [==============================] - 15s 247ms/step - loss: 1.2690 - lr: 0.0010\n","Epoch 77/150\n","61/61 [==============================] - ETA: 0s - loss: 1.2749\n","Epoch 77: loss did not improve from 1.26900\n","61/61 [==============================] - 14s 234ms/step - loss: 1.2749 - lr: 0.0010\n","Epoch 78/150\n","61/61 [==============================] - ETA: 0s - loss: 1.2640\n","Epoch 78: loss improved from 1.26900 to 1.26399, saving model to nextword1.h5\n","61/61 [==============================] - 15s 245ms/step - loss: 1.2640 - lr: 0.0010\n","Epoch 79/150\n","61/61 [==============================] - ETA: 0s - loss: 1.2507\n","Epoch 79: loss improved from 1.26399 to 1.25072, saving model to nextword1.h5\n","61/61 [==============================] - 15s 243ms/step - loss: 1.2507 - lr: 0.0010\n","Epoch 80/150\n","61/61 [==============================] - ETA: 0s - loss: 1.2372\n","Epoch 80: loss improved from 1.25072 to 1.23717, saving model to nextword1.h5\n","61/61 [==============================] - 16s 265ms/step - loss: 1.2372 - lr: 0.0010\n","Epoch 81/150\n","61/61 [==============================] - ETA: 0s - loss: 1.2618\n","Epoch 81: loss did not improve from 1.23717\n","61/61 [==============================] - 14s 238ms/step - loss: 1.2618 - lr: 0.0010\n","Epoch 82/150\n","61/61 [==============================] - ETA: 0s - loss: 1.2393\n","Epoch 82: loss did not improve from 1.23717\n","61/61 [==============================] - 15s 238ms/step - loss: 1.2393 - lr: 0.0010\n","Epoch 83/150\n","61/61 [==============================] - ETA: 0s - loss: 1.2169\n","Epoch 83: loss improved from 1.23717 to 1.21690, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 1.2169 - lr: 0.0010\n","Epoch 84/150\n","61/61 [==============================] - ETA: 0s - loss: 1.2050\n","Epoch 84: loss improved from 1.21690 to 1.20503, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 1.2050 - lr: 0.0010\n","Epoch 85/150\n","61/61 [==============================] - ETA: 0s - loss: 1.2025\n","Epoch 85: loss improved from 1.20503 to 1.20252, saving model to nextword1.h5\n","61/61 [==============================] - 15s 240ms/step - loss: 1.2025 - lr: 0.0010\n","Epoch 86/150\n","61/61 [==============================] - ETA: 0s - loss: 1.1734\n","Epoch 86: loss improved from 1.20252 to 1.17340, saving model to nextword1.h5\n","61/61 [==============================] - 15s 248ms/step - loss: 1.1734 - lr: 0.0010\n","Epoch 87/150\n","61/61 [==============================] - ETA: 0s - loss: 1.1612\n","Epoch 87: loss improved from 1.17340 to 1.16116, saving model to nextword1.h5\n","61/61 [==============================] - 16s 267ms/step - loss: 1.1612 - lr: 0.0010\n","Epoch 88/150\n","61/61 [==============================] - ETA: 0s - loss: 1.1523\n","Epoch 88: loss improved from 1.16116 to 1.15233, saving model to nextword1.h5\n","61/61 [==============================] - 15s 239ms/step - loss: 1.1523 - lr: 0.0010\n","Epoch 89/150\n","61/61 [==============================] - ETA: 0s - loss: 1.1425\n","Epoch 89: loss improved from 1.15233 to 1.14247, saving model to nextword1.h5\n","61/61 [==============================] - 15s 247ms/step - loss: 1.1425 - lr: 0.0010\n","Epoch 90/150\n","61/61 [==============================] - ETA: 0s - loss: 1.1094\n","Epoch 90: loss improved from 1.14247 to 1.10936, saving model to nextword1.h5\n","61/61 [==============================] - 15s 243ms/step - loss: 1.1094 - lr: 0.0010\n","Epoch 91/150\n","61/61 [==============================] - ETA: 0s - loss: 1.1076\n","Epoch 91: loss improved from 1.10936 to 1.10755, saving model to nextword1.h5\n","61/61 [==============================] - 15s 249ms/step - loss: 1.1076 - lr: 0.0010\n","Epoch 92/150\n","61/61 [==============================] - ETA: 0s - loss: 1.1072\n","Epoch 92: loss improved from 1.10755 to 1.10716, saving model to nextword1.h5\n","61/61 [==============================] - 15s 245ms/step - loss: 1.1072 - lr: 0.0010\n","Epoch 93/150\n","61/61 [==============================] - ETA: 0s - loss: 1.1019\n","Epoch 93: loss improved from 1.10716 to 1.10191, saving model to nextword1.h5\n","61/61 [==============================] - 15s 247ms/step - loss: 1.1019 - lr: 0.0010\n","Epoch 94/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0860\n","Epoch 94: loss improved from 1.10191 to 1.08604, saving model to nextword1.h5\n","61/61 [==============================] - 16s 263ms/step - loss: 1.0860 - lr: 0.0010\n","Epoch 95/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0889\n","Epoch 95: loss did not improve from 1.08604\n","61/61 [==============================] - 14s 236ms/step - loss: 1.0889 - lr: 0.0010\n","Epoch 96/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0757\n","Epoch 96: loss improved from 1.08604 to 1.07567, saving model to nextword1.h5\n","61/61 [==============================] - 15s 248ms/step - loss: 1.0757 - lr: 0.0010\n","Epoch 97/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0687\n","Epoch 97: loss improved from 1.07567 to 1.06871, saving model to nextword1.h5\n","61/61 [==============================] - 15s 249ms/step - loss: 1.0687 - lr: 0.0010\n","Epoch 98/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0621\n","Epoch 98: loss improved from 1.06871 to 1.06208, saving model to nextword1.h5\n","61/61 [==============================] - 15s 247ms/step - loss: 1.0621 - lr: 0.0010\n","Epoch 99/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0649\n","Epoch 99: loss did not improve from 1.06208\n","61/61 [==============================] - 15s 247ms/step - loss: 1.0649 - lr: 0.0010\n","Epoch 100/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0585\n","Epoch 100: loss improved from 1.06208 to 1.05849, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 1.0585 - lr: 0.0010\n","Epoch 101/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0785\n","Epoch 101: loss did not improve from 1.05849\n","61/61 [==============================] - 14s 236ms/step - loss: 1.0785 - lr: 0.0010\n","Epoch 102/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0587\n","Epoch 102: loss did not improve from 1.05849\n","61/61 [==============================] - 16s 256ms/step - loss: 1.0587 - lr: 0.0010\n","Epoch 103/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0540\n","Epoch 103: loss improved from 1.05849 to 1.05396, saving model to nextword1.h5\n","61/61 [==============================] - 15s 252ms/step - loss: 1.0540 - lr: 0.0010\n","Epoch 104/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0602\n","Epoch 104: loss did not improve from 1.05396\n","61/61 [==============================] - 14s 237ms/step - loss: 1.0602 - lr: 0.0010\n","Epoch 105/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0380\n","Epoch 105: loss improved from 1.05396 to 1.03795, saving model to nextword1.h5\n","61/61 [==============================] - 15s 248ms/step - loss: 1.0380 - lr: 0.0010\n","Epoch 106/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0251\n","Epoch 106: loss improved from 1.03795 to 1.02513, saving model to nextword1.h5\n","61/61 [==============================] - 15s 248ms/step - loss: 1.0251 - lr: 0.0010\n","Epoch 107/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0367\n","Epoch 107: loss did not improve from 1.02513\n","61/61 [==============================] - 14s 237ms/step - loss: 1.0367 - lr: 0.0010\n","Epoch 108/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0714\n","Epoch 108: loss did not improve from 1.02513\n","61/61 [==============================] - 15s 240ms/step - loss: 1.0714 - lr: 0.0010\n","Epoch 109/150\n","61/61 [==============================] - ETA: 0s - loss: 1.0918\n","Epoch 109: loss did not improve from 1.02513\n","\n","Epoch 109: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","61/61 [==============================] - 15s 251ms/step - loss: 1.0918 - lr: 0.0010\n","Epoch 110/150\n","61/61 [==============================] - ETA: 0s - loss: 0.8307\n","Epoch 110: loss improved from 1.02513 to 0.83066, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 0.8307 - lr: 2.0000e-04\n","Epoch 111/150\n","61/61 [==============================] - ETA: 0s - loss: 0.7271\n","Epoch 111: loss improved from 0.83066 to 0.72709, saving model to nextword1.h5\n","61/61 [==============================] - 16s 260ms/step - loss: 0.7271 - lr: 2.0000e-04\n","Epoch 112/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6992\n","Epoch 112: loss improved from 0.72709 to 0.69919, saving model to nextword1.h5\n","61/61 [==============================] - 17s 269ms/step - loss: 0.6992 - lr: 2.0000e-04\n","Epoch 113/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6870\n","Epoch 113: loss improved from 0.69919 to 0.68701, saving model to nextword1.h5\n","61/61 [==============================] - 15s 250ms/step - loss: 0.6870 - lr: 2.0000e-04\n","Epoch 114/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6803\n","Epoch 114: loss improved from 0.68701 to 0.68028, saving model to nextword1.h5\n","61/61 [==============================] - 15s 248ms/step - loss: 0.6803 - lr: 2.0000e-04\n","Epoch 115/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6769\n","Epoch 115: loss improved from 0.68028 to 0.67694, saving model to nextword1.h5\n","61/61 [==============================] - 15s 247ms/step - loss: 0.6769 - lr: 2.0000e-04\n","Epoch 116/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6728\n","Epoch 116: loss improved from 0.67694 to 0.67283, saving model to nextword1.h5\n","61/61 [==============================] - 16s 263ms/step - loss: 0.6728 - lr: 2.0000e-04\n","Epoch 117/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6720\n","Epoch 117: loss improved from 0.67283 to 0.67195, saving model to nextword1.h5\n","61/61 [==============================] - 15s 243ms/step - loss: 0.6720 - lr: 2.0000e-04\n","Epoch 118/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6711\n","Epoch 118: loss improved from 0.67195 to 0.67108, saving model to nextword1.h5\n","61/61 [==============================] - 15s 245ms/step - loss: 0.6711 - lr: 2.0000e-04\n","Epoch 119/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6698\n","Epoch 119: loss improved from 0.67108 to 0.66985, saving model to nextword1.h5\n","61/61 [==============================] - 15s 248ms/step - loss: 0.6698 - lr: 2.0000e-04\n","Epoch 120/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6688\n","Epoch 120: loss improved from 0.66985 to 0.66881, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 0.6688 - lr: 2.0000e-04\n","Epoch 121/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6682\n","Epoch 121: loss improved from 0.66881 to 0.66823, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 0.6682 - lr: 2.0000e-04\n","Epoch 122/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6673\n","Epoch 122: loss improved from 0.66823 to 0.66728, saving model to nextword1.h5\n","61/61 [==============================] - 15s 245ms/step - loss: 0.6673 - lr: 2.0000e-04\n","Epoch 123/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6673\n","Epoch 123: loss did not improve from 0.66728\n","61/61 [==============================] - 15s 251ms/step - loss: 0.6673 - lr: 2.0000e-04\n","Epoch 124/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6676\n","Epoch 124: loss did not improve from 0.66728\n","61/61 [==============================] - 14s 232ms/step - loss: 0.6676 - lr: 2.0000e-04\n","Epoch 125/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6663\n","Epoch 125: loss improved from 0.66728 to 0.66632, saving model to nextword1.h5\n","61/61 [==============================] - 15s 249ms/step - loss: 0.6663 - lr: 2.0000e-04\n","Epoch 126/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6663\n","Epoch 126: loss did not improve from 0.66632\n","61/61 [==============================] - 14s 231ms/step - loss: 0.6663 - lr: 2.0000e-04\n","Epoch 127/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6662\n","Epoch 127: loss improved from 0.66632 to 0.66618, saving model to nextword1.h5\n","61/61 [==============================] - 15s 244ms/step - loss: 0.6662 - lr: 2.0000e-04\n","Epoch 128/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6660\n","Epoch 128: loss improved from 0.66618 to 0.66604, saving model to nextword1.h5\n","61/61 [==============================] - 15s 243ms/step - loss: 0.6660 - lr: 2.0000e-04\n","Epoch 129/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6661\n","Epoch 129: loss did not improve from 0.66604\n","61/61 [==============================] - 14s 229ms/step - loss: 0.6661 - lr: 2.0000e-04\n","Epoch 130/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6665\n","Epoch 130: loss did not improve from 0.66604\n","61/61 [==============================] - 14s 234ms/step - loss: 0.6665 - lr: 2.0000e-04\n","Epoch 131/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6654\n","Epoch 131: loss improved from 0.66604 to 0.66544, saving model to nextword1.h5\n","61/61 [==============================] - 16s 259ms/step - loss: 0.6654 - lr: 2.0000e-04\n","Epoch 132/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6648\n","Epoch 132: loss improved from 0.66544 to 0.66479, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 0.6648 - lr: 2.0000e-04\n","Epoch 133/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6654\n","Epoch 133: loss did not improve from 0.66479\n","61/61 [==============================] - 14s 232ms/step - loss: 0.6654 - lr: 2.0000e-04\n","Epoch 134/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6643\n","Epoch 134: loss improved from 0.66479 to 0.66433, saving model to nextword1.h5\n","61/61 [==============================] - 15s 240ms/step - loss: 0.6643 - lr: 2.0000e-04\n","Epoch 135/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6637\n","Epoch 135: loss improved from 0.66433 to 0.66365, saving model to nextword1.h5\n","61/61 [==============================] - 15s 247ms/step - loss: 0.6637 - lr: 2.0000e-04\n","Epoch 136/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6633\n","Epoch 136: loss improved from 0.66365 to 0.66333, saving model to nextword1.h5\n","61/61 [==============================] - 15s 243ms/step - loss: 0.6633 - lr: 2.0000e-04\n","Epoch 137/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6637\n","Epoch 137: loss did not improve from 0.66333\n","61/61 [==============================] - 14s 232ms/step - loss: 0.6637 - lr: 2.0000e-04\n","Epoch 138/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6624\n","Epoch 138: loss improved from 0.66333 to 0.66243, saving model to nextword1.h5\n","61/61 [==============================] - 16s 256ms/step - loss: 0.6624 - lr: 2.0000e-04\n","Epoch 139/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6635\n","Epoch 139: loss did not improve from 0.66243\n","61/61 [==============================] - 14s 233ms/step - loss: 0.6635 - lr: 2.0000e-04\n","Epoch 140/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6630\n","Epoch 140: loss did not improve from 0.66243\n","61/61 [==============================] - 14s 232ms/step - loss: 0.6630 - lr: 2.0000e-04\n","Epoch 141/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6645\n","Epoch 141: loss did not improve from 0.66243\n","\n","Epoch 141: ReduceLROnPlateau reducing learning rate to 0.0001.\n","61/61 [==============================] - 14s 230ms/step - loss: 0.6645 - lr: 2.0000e-04\n","Epoch 142/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6246\n","Epoch 142: loss improved from 0.66243 to 0.62459, saving model to nextword1.h5\n","61/61 [==============================] - 15s 241ms/step - loss: 0.6246 - lr: 1.0000e-04\n","Epoch 143/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6226\n","Epoch 143: loss improved from 0.62459 to 0.62257, saving model to nextword1.h5\n","61/61 [==============================] - 14s 237ms/step - loss: 0.6226 - lr: 1.0000e-04\n","Epoch 144/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6223\n","Epoch 144: loss improved from 0.62257 to 0.62232, saving model to nextword1.h5\n","61/61 [==============================] - 15s 240ms/step - loss: 0.6223 - lr: 1.0000e-04\n","Epoch 145/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6217\n","Epoch 145: loss improved from 0.62232 to 0.62175, saving model to nextword1.h5\n","61/61 [==============================] - 15s 240ms/step - loss: 0.6217 - lr: 1.0000e-04\n","Epoch 146/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6217\n","Epoch 146: loss improved from 0.62175 to 0.62171, saving model to nextword1.h5\n","61/61 [==============================] - 16s 261ms/step - loss: 0.6217 - lr: 1.0000e-04\n","Epoch 147/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6211\n","Epoch 147: loss improved from 0.62171 to 0.62111, saving model to nextword1.h5\n","61/61 [==============================] - 15s 242ms/step - loss: 0.6211 - lr: 1.0000e-04\n","Epoch 148/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6224\n","Epoch 148: loss did not improve from 0.62111\n","61/61 [==============================] - 14s 230ms/step - loss: 0.6224 - lr: 1.0000e-04\n","Epoch 149/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6218\n","Epoch 149: loss did not improve from 0.62111\n","61/61 [==============================] - 14s 229ms/step - loss: 0.6218 - lr: 1.0000e-04\n","Epoch 150/150\n","61/61 [==============================] - ETA: 0s - loss: 0.6211\n","Epoch 150: loss improved from 0.62111 to 0.62109, saving model to nextword1.h5\n","61/61 [==============================] - 15s 241ms/step - loss: 0.6211 - lr: 1.0000e-04\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f78d88aea50\u003e"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"]},{"cell_type":"markdown","metadata":{"id":"b29ZceuIlm9r"},"source":["### Graph:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CLbN3je5lm9r"},"outputs":[],"source":["# https://stackoverflow.com/questions/26649716/how-to-show-pil-image-in-ipython-notebook\n","# tensorboard --logdir=\"./logsnextword1\"\n","# http://DESKTOP-U3TSCVT:6006/\n","\n","from IPython.display import Image \n","#pil_img = Image(filename='graph1.png')\n","#display(pil_img)"]},{"cell_type":"markdown","metadata":{"id":"vnnxy_gdlm9s"},"source":["## Observation:\n","### We are able to develop a decent next word prediction model and are able to get a declining loss and an overall decent performance."]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}